# Mini Search
A simple POC search engine to learn about Search Technologies.
https://mini-search-d9465.web.app/

## Description
A mini search engine specifically designed for searching programming documentation using existing technologies. Uses
- [tantivy](https://github.com/quickwit-oss/tantivy) for search,
- [voyager](https://github.com/mattsse/voyager) for crawling websites
- [axum](https://github.com/tokio-rs/axum) for APIs
- [vite](https://vite.dev/) and [React](https://react.dev) for UI
- [shuttle](https://shuttle.dev) for API deployment
- [firebase](https://firebase.google.com/) for UI deployment

## Getting Started

### Dependencies

* Rust 1.81
* Node 18 ( npm 9 )

### Structure
The codebase is set up as a mono repo with the following structure
- crates ( Modular building blocks which can be substituted easily )
	- crawler - The crawler component which visits websites
	- searcher - The component responsible for searching and returning results
	- config - Common config used throughout the project
- apps 
	- api - APIs built using Axum to serve search, analytics and crawling triggers
	- search-ui - React App for Search and analytics page
- index ( Search index generated by crawler & used by searcher which stores all sitedata )

### Setup - UI
- Navigate to the `search-ui` directory
`cd apps/search-ui`
- Install dependencies
`npm install`
- Run the app
`npm run dev`
_You may need to update the baseURL in `App.tsx` to point to your backend_

### Setup - Core
- Install dependencies
`cargo build`
- Install shuttle ( https://docs.shuttle.rs/getting-started/installation )
- Run the app
`shuttle run`

*Note: To test out an independent crates you can also navigate to the crate and run the corresponding examples*

### Creating the Index
- Ensure that the `~/index` directory exists. This is where the index is stored
- The file `crates/config/src` contains the following config variables
	- `SITES` - list of sites to be scanned
	- `NUM_PAGES_PER_SITE` - number of pages to be indexed for each domain
	- `RE_CRAWL_DURATION` - duration after which a page data will be updated and re-indexed
- Update these values to suitable values
- Run the `create-index` example
`cargo run --example create_index`
- Alternatively, you can also start the API server and call the `trigger_index` endpoint


## Next Steps

### System Improvements
- Allow deploying the search index remotely in a distributed fashion
- Fine tune scraper and searcher to search for documentation
- Reduce search latency
	- Reduce size of indexed data
- Add more entrypoints for crawling supported sites

### Code
-   Better logging